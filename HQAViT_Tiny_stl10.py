"""
HQA-ViT STL-10 Semi-Supervised Learning with SimCLR v2 (FIXED & OPTIMIZED)
Phase 1: SimCLR v2 self-supervised pre-training on 100k unlabeled images
Phase 2: Fine-tuning on 5k labeled images
Testing robustness of CIFAR-100 pretrained model

FIXES:
- All operations use BF16 for stability
- Conservative learning rate with proper warmup
- Stable NT-Xent loss implementation
- Gradient clipping and NaN detection
- Optimized hyperparameters for small batch size
"""

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
import warnings
warnings.filterwarnings('ignore')

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.cuda.amp import GradScaler, autocast
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import datasets, transforms
import time 
from pathlib import Path
from dataclasses import dataclass
import numpy as np
import matplotlib.pyplot as plt
import json
from datetime import datetime
from typing import Tuple
import math

try:
    from HQAViT_CIFAR100 import HQAViT, HQAViTConfig
except ImportError:
    print("Error: Cannot import model classes. Make sure HQAViT_CIFAR100.py is in the same directory.")
    raise


@dataclass
class SimCLRConfig:
    """SimCLR v2 configuration - Optimized for stability"""
    # SimCLR training
    simclr_epochs: int = 100
    simclr_batch_size: int = 64  # Small batch size for laptop GPU
    simclr_base_lr: float = 0.05  # Reduced from 0.3 for stability
    simclr_warmup_epochs: int = 10
    simclr_temperature: float = 0.1  # Increased from 0.07 for stability
    
    # SimCLR v2 projection head
    projection_dim: int = 128
    hidden_dim: int = 2048
    
    # Fine-tuning
    finetune_epochs: int = 100
    finetune_batch_size: int = 128
    finetune_base_lr: float = 3e-5  # Reduced for stability
    finetune_warmup_epochs: int = 5
    
    # Common settings - ALL USE BF16
    num_workers: int = 2
    pin_memory: bool = True
    weight_decay: float = 1e-4
    max_grad_norm: float = 1.0
    use_amp: bool = True
    amp_dtype: str = 'bfloat16'  # BF16 for stability
    
    # Image settings
    img_size: int = 96
    resize_to: int = 96
    
    # Paths
    pretrained_path: str = "./checkpoints_finetuned/best_finetuned.pth"
    data_root: str = "./data"
    checkpoint_dir: str = "./checkpoints_stl10_simclr"
    log_dir: str = "./logs_stl10_simclr"
    
    print_freq: int = 20
    eval_freq: int = 1
    save_freq: int = 20


@dataclass
class FineTuneConfig:
    batch_size: int = 128
    num_workers: int = 2
    pin_memory: bool = True
    
    epochs: int = 100
    warmup_epochs: int = 5
    
    base_lr: float = 5e-5  # Reduced for stability
    head_lr_multiplier: float = 10.0
    min_lr: float = 1e-6
    
    weight_decay: float = 0.05
    label_smoothing: float = 0.1
    
    max_grad_norm: float = 1.0
    
    use_amp: bool = True
    amp_dtype: str = 'bfloat16'  # BF16 for stability
    
    pretrained_path: str = "./checkpoints_finetuned/best_finetuned.pth"
    data_root: str = "./data"
    checkpoint_dir: str = "./checkpoints_stl10"
    log_dir: str = "./logs_stl10"
    
    print_freq: int = 50
    eval_freq: int = 1
    save_freq: int = 20


class SimCLRProjectionHead(nn.Module):
    """SimCLR v2 projection head with 3 layers"""
    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):
        super().__init__()
        self.projection = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, output_dim),
        )
    
    def forward(self, x):
        return self.projection(x)


class HQAViTWithSimCLR(nn.Module):
    """HQA-ViT with SimCLR projection head"""
    def __init__(self, backbone: HQAViT, config: SimCLRConfig):
        super().__init__()
        self.backbone = backbone
        
        # Get embedding dimension from backbone
        self.embedding_dim = backbone.head.in_features if hasattr(backbone.head, 'in_features') else 768
        
        # SimCLR v2 projection head
        self.projection_head = SimCLRProjectionHead(
            input_dim=self.embedding_dim,
            hidden_dim=config.hidden_dim,
            output_dim=config.projection_dim
        )
    
    def forward(self, x):
        # Get features from backbone (before classification head)
        features = self._extract_features(x)
        
        # Project to SimCLR space
        projections = self.projection_head(features)
        # Normalize with eps for numerical stability
        return F.normalize(projections, dim=1, eps=1e-6)
    
    def _extract_features(self, x):
        """Extract features before classification head"""
        img = x

        # Extract CNN lateral features
        F2, F3, F4 = self.backbone.cnn_stem(img)

        # Adapt CNN features to token embedding space
        A2 = self.backbone.lmfa2(F2)
        A3 = self.backbone.lmfa3(F3)
        A4 = self.backbone.lmfa4(F4)

        # Refine adapted tokens through RRCV modules
        R2 = self.backbone.rrcv2(A2, self.backbone.H, self.backbone.W)
        R3 = self.backbone.rrcv3(A3, self.backbone.H, self.backbone.W)
        R4 = self.backbone.rrcv4(A4, self.backbone.H, self.backbone.W)

        # ViT path: Patch embedding from image
        T = self.backbone.patch_embed(img)

        # Handle positional embedding size mismatch
        pos = self.backbone.pos_embed
        if pos.shape[1] != T.shape[1]:
            old_N = pos.shape[1]
            new_N = T.shape[1]
            old_size = int(math.sqrt(old_N))
            new_size = int(math.sqrt(new_N))
            if old_size * old_size == old_N and new_size * new_size == new_N:
                pos_reshaped = pos.reshape(1, old_size, old_size, pos.shape[2]).permute(0, 3, 1, 2)
                pos_resized = F.interpolate(pos_reshaped, size=(new_size, new_size), mode='bilinear', align_corners=False)
                pos = pos_resized.permute(0, 2, 3, 1).reshape(1, new_N, pos.shape[2])
            else:
                if new_N > old_N:
                    repeats = new_N // old_N + 1
                    pos = pos.repeat(1, repeats, 1)[:, :new_N, :]
                else:
                    pos = pos[:, :new_N, :]

        T = T + pos.to(T.device)
        T = self.backbone.pos_drop(T)

        # Stage 1
        for block in getattr(self.backbone, 'stage1_blocks', []):
            T = block(T)

        # Stage 2: fuse with R2
        if hasattr(self.backbone, 'fuse2'):
            T = self.backbone.fuse2(T, R2)
        for block in getattr(self.backbone, 'stage2_blocks', []):
            T = block(T)

        # Stage 3: fuse with R3
        if hasattr(self.backbone, 'fuse3'):
            T = self.backbone.fuse3(T, R3)
        for block in getattr(self.backbone, 'stage3_blocks', []):
            T = block(T)

        # Stage 4: fuse with R4
        if hasattr(self.backbone, 'fuse4'):
            T = self.backbone.fuse4(T, R4)
        for block in getattr(self.backbone, 'stage4_blocks', []):
            T = block(T)

        # Final normalization + pooled features
        T = self.backbone.norm(T)
        features = T.mean(dim=1)
        return features


class SimCLRAugmentation:
    """SimCLR v2 augmentation pipeline"""
    def __init__(self, size: int = 96):
        # SimCLR augmentation: strong color distortion + random crop + flip
        self.transform = transforms.Compose([
            transforms.RandomResizedCrop(size, scale=(0.2, 1.0)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomApply([
                transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)
            ], p=0.8),
            transforms.RandomGrayscale(p=0.2),
            transforms.RandomApply([
                transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))
            ], p=0.5),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=(0.4467, 0.4398, 0.4066),
                std=(0.2603, 0.2566, 0.2713)
            ),
        ])
    
    def __call__(self, x):
        """Return two augmented views"""
        return self.transform(x), self.transform(x)


class ContrastiveDataset(Dataset):
    """Dataset wrapper for contrastive learning"""
    def __init__(self, base_dataset, transform):
        self.base_dataset = base_dataset
        self.transform = transform
    
    def __len__(self):
        return len(self.base_dataset)
    
    def __getitem__(self, idx):
        img, _ = self.base_dataset[idx]  # Ignore labels
        view1, view2 = self.transform(img)
        return view1, view2


class NTXentLoss(nn.Module):
    """Normalized Temperature-scaled Cross Entropy Loss (STABLE VERSION)"""
    def __init__(self, temperature: float = 0.1):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, z_i: torch.Tensor, z_j: torch.Tensor) -> torch.Tensor:
        """
        z_i, z_j: [batch_size, projection_dim] - normalized projections
        """
        batch_size = z_i.shape[0]
        
        # Concatenate both views: [2*batch_size, projection_dim]
        z = torch.cat([z_i, z_j], dim=0)
        
        # Compute similarity matrix: [2*batch_size, 2*batch_size]
        sim_matrix = torch.mm(z, z.T) / self.temperature
        
        # Create positive pairs mask
        # For each sample i, positive is i+batch_size
        mask = torch.eye(batch_size, dtype=torch.bool, device=z.device)
        pos_mask = torch.cat([
            torch.cat([mask, mask], dim=1),
            torch.cat([mask, mask], dim=1)
        ], dim=0)
        
        # Create negative mask (all except self and positive)
        neg_mask = ~torch.eye(2 * batch_size, dtype=torch.bool, device=z.device)
        
        # Extract positive similarities
        pos_sim = sim_matrix[pos_mask].view(2 * batch_size, -1)
        
        # Extract negative similarities
        neg_sim = sim_matrix[neg_mask].view(2 * batch_size, -1)
        
        # Compute log-softmax using logsumexp for numerical stability
        # Concatenate positive and negative similarities
        logits = torch.cat([pos_sim, neg_sim], dim=1)
        
        # Labels: positive is always first
        labels = torch.zeros(2 * batch_size, dtype=torch.long, device=z.device)
        
        # Cross entropy loss (more stable than manual computation)
        loss = F.cross_entropy(logits / self.temperature, labels)
        
        return loss


class TrainingLogger:
    """Comprehensive training logger"""
    def __init__(self, log_dir, phase='simclr'):
        self.log_dir = Path(log_dir) / phase
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.phase = phase
        
        self.history = {
            'epoch': [],
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'lr': [],
            'epoch_time': []
        }
        
        self.best_val_acc = 0.0
        self.best_loss = float('inf')
        self.start_time = time.time()
    
    def log_epoch(self, epoch, metrics):
        self.history['epoch'].append(epoch)
        for key, value in metrics.items():
            if key in self.history:
                self.history[key].append(value)
        
        if 'val_acc' in metrics and metrics['val_acc'] > self.best_val_acc:
            self.best_val_acc = metrics['val_acc']
        if 'train_loss' in metrics and metrics['train_loss'] < self.best_loss:
            self.best_loss = metrics['train_loss']
    
    def save_metrics(self):
        metrics_file = self.log_dir / 'training_metrics.json'
        with open(metrics_file, 'w') as f:
            json.dump({
                'history': self.history,
                'best_val_acc': self.best_val_acc,
                'best_loss': self.best_loss,
                'total_time': time.time() - self.start_time
            }, f, indent=2)
        print(f"üìä Metrics saved to: {metrics_file}")
    
    def plot_training_curves(self):
        if len(self.history['epoch']) == 0:
            return
        
        if self.phase == 'simclr':
            self._plot_simclr_curves()
        else:
            self._plot_finetune_curves()
    
    def _plot_simclr_curves(self):
        """Plot SimCLR pre-training curves"""
        fig = plt.figure(figsize=(16, 6))
        
        # Loss curve
        ax1 = plt.subplot(1, 2, 1)
        ax1.plot(self.history['epoch'], self.history['train_loss'], 'b-', 
                label='Contrastive Loss', linewidth=2.5, marker='o', markersize=4)
        ax1.axhline(y=self.best_loss, color='g', linestyle='--', 
                   linewidth=2, label=f'Best Loss: {self.best_loss:.4f}')
        ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')
        ax1.set_ylabel('NT-Xent Loss', fontsize=12, fontweight='bold')
        ax1.set_title('SimCLR v2 Self-Supervised Pre-training', fontsize=14, fontweight='bold')
        ax1.legend(fontsize=11)
        ax1.grid(True, alpha=0.3)
        
        # Learning rate
        ax2 = plt.subplot(1, 2, 2)
        ax2.plot(self.history['epoch'], self.history['lr'], 'purple', linewidth=2.5)
        ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')
        ax2.set_ylabel('Learning Rate', fontsize=12, fontweight='bold')
        ax2.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
        ax2.set_yscale('log')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plot_path = self.log_dir / 'simclr_training.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"üìä SimCLR curves saved to: {plot_path}")
    
    def _plot_finetune_curves(self):
        """Plot fine-tuning curves"""
        fig = plt.figure(figsize=(20, 6))
        
        ax1 = plt.subplot(1, 3, 1)
        ax1.plot(self.history['epoch'], self.history['train_loss'], 'b-', 
                label='Train Loss', linewidth=2, marker='o', markersize=3)
        ax1.plot(self.history['epoch'], self.history['val_loss'], 'r-', 
                label='Val Loss', linewidth=2, marker='s', markersize=3)
        ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Loss', fontsize=12, fontweight='bold')
        ax1.set_title('Fine-tuning Loss', fontsize=14, fontweight='bold')
        ax1.legend(fontsize=11)
        ax1.grid(True, alpha=0.3)
        
        ax2 = plt.subplot(1, 3, 2)
        ax2.plot(self.history['epoch'], self.history['train_acc'], 'b-', 
                label='Train Acc', linewidth=2, marker='o', markersize=3)
        ax2.plot(self.history['epoch'], self.history['val_acc'], 'r-', 
                label='Val Acc', linewidth=2, marker='s', markersize=3)
        ax2.axhline(y=self.best_val_acc, color='g', linestyle='--', 
                   linewidth=2, label=f'Best: {self.best_val_acc:.2f}%')
        ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')
        ax2.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')
        ax2.set_title('Fine-tuning Accuracy', fontsize=14, fontweight='bold')
        ax2.legend(fontsize=11)
        ax2.grid(True, alpha=0.3)
        
        ax3 = plt.subplot(1, 3, 3)
        ax3.plot(self.history['epoch'], self.history['lr'], 'purple', linewidth=2.5)
        ax3.set_xlabel('Epoch', fontsize=12, fontweight='bold')
        ax3.set_ylabel('Learning Rate', fontsize=12, fontweight='bold')
        ax3.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
        ax3.set_yscale('log')
        ax3.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plot_path = self.log_dir / 'finetune_training.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"üìä Fine-tuning curves saved to: {plot_path}")


def get_stl10_unlabeled_loader(config: SimCLRConfig):
    """Get STL-10 unlabeled data (100k images) for SimCLR"""
    base_dataset = datasets.STL10(
        root=config.data_root,
        split='unlabeled',
        transform=transforms.Compose([
            transforms.Resize(config.resize_to + 8),
            transforms.CenterCrop(config.resize_to + 8),
        ]),
        download=True
    )
    
    augmentation = SimCLRAugmentation(size=config.resize_to)
    contrastive_dataset = ContrastiveDataset(base_dataset, augmentation)
    
    loader = DataLoader(
        contrastive_dataset,
        batch_size=config.simclr_batch_size,
        shuffle=True,
        num_workers=config.num_workers,
        pin_memory=config.pin_memory,
        drop_last=True,
        persistent_workers=True if config.num_workers > 0 else False,
    )
    
    return loader


def get_stl10_supervised_loaders(config: SimCLRConfig):
    """Get STL-10 labeled data (5k train, 8k test) for fine-tuning"""
    mean = (0.4467, 0.4398, 0.4066)
    std = (0.2603, 0.2566, 0.2713)
    
    train_transform = transforms.Compose([
        transforms.Resize(config.resize_to + 8),
        transforms.RandomCrop(config.resize_to, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean, std),
        transforms.RandomErasing(p=0.3, scale=(0.02, 0.2), ratio=(0.3, 3.3))
    ])
    
    val_transform = transforms.Compose([
        transforms.Resize(config.resize_to),
        transforms.ToTensor(),
        transforms.Normalize(mean, std),
    ])
    
    train_dataset = datasets.STL10(
        root=config.data_root,
        split='train',
        transform=train_transform,
        download=True
    )
    
    val_dataset = datasets.STL10(
        root=config.data_root,
        split='test',
        transform=val_transform,
        download=True
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.finetune_batch_size,
        shuffle=True,
        num_workers=config.num_workers,
        pin_memory=config.pin_memory,
        drop_last=True,
        persistent_workers=True if config.num_workers > 0 else False,
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.finetune_batch_size * 2,
        shuffle=False,
        num_workers=config.num_workers,
        pin_memory=config.pin_memory,
        persistent_workers=True if config.num_workers > 0 else False,
    )
    
    return train_loader, val_loader


def check_for_nan(loss, model, optimizer, epoch, batch_idx):
    """Check for NaN and provide debugging info"""
    if torch.isnan(loss) or torch.isinf(loss):
        print(f"\n‚ùå NaN/Inf detected at epoch {epoch}, batch {batch_idx}")
        print(f"   Loss value: {loss.item()}")
        
        # Check model parameters
        nan_params = []
        inf_params = []
        for name, param in model.named_parameters():
            if param.grad is not None:
                if torch.isnan(param.grad).any():
                    nan_params.append(name)
                if torch.isinf(param.grad).any():
                    inf_params.append(name)
        
        if nan_params:
            print(f"   Parameters with NaN gradients: {nan_params[:5]}")
        if inf_params:
            print(f"   Parameters with Inf gradients: {inf_params[:5]}")
        
        print("   Stopping training to prevent further corruption.")
        return True
    return False


def train_simclr_epoch(model, loader, optimizer, scheduler, scaler, criterion, config, epoch):
    """Train one epoch of SimCLR with stability checks"""
    model.train()
    total_loss = 0
    num_batches = 0
    
    # Use BF16
    amp_dtype = torch.bfloat16
    
    for batch_idx, (view1, view2) in enumerate(loader):
        view1, view2 = view1.cuda(), view2.cuda()
        
        optimizer.zero_grad()
        
        with autocast(enabled=config.use_amp, dtype=amp_dtype):
            # Get projections for both views
            z1 = model(view1)
            z2 = model(view2)
            
            # Compute contrastive loss
            loss = criterion(z1, z2)
        
        # Check for NaN before backward
        if check_for_nan(loss, model, optimizer, epoch, batch_idx):
            raise ValueError("Training stopped due to NaN loss")
        
        # Backward pass with gradient scaling
        scaler.scale(loss).backward()
        
        # Unscale gradients before clipping
        scaler.unscale_(optimizer)
        
        # Clip gradients
        torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)
        
        # Optimizer step
        scaler.step(optimizer)
        scaler.update()
        
        total_loss += loss.item()
        num_batches += 1
        
        if batch_idx % config.print_freq == 0:
            lr = optimizer.param_groups[0]['lr']
            avg_loss = total_loss / num_batches
            print(f'  Epoch {epoch:3d} [{batch_idx:4d}/{len(loader):4d}] | '
                  f'Loss: {avg_loss:.4f} | LR: {lr:.6f}')
    
    scheduler.step()
    return total_loss / num_batches


def train_finetune_epoch(model, loader, optimizer, scheduler, scaler, criterion, config, epoch):
    """Train one epoch of supervised fine-tuning"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    num_batches = 0
    
    # Use BF16
    amp_dtype = torch.bfloat16
    
    for batch_idx, (inputs, targets) in enumerate(loader):
        inputs, targets = inputs.cuda(), targets.cuda()
        
        optimizer.zero_grad()
        
        with autocast(enabled=config.use_amp, dtype=amp_dtype):
            outputs = model(inputs)
            loss = criterion(outputs, targets)
        
        # Check for NaN
        if check_for_nan(loss, model, optimizer, epoch, batch_idx):
            raise ValueError("Training stopped due to NaN loss")
        
        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)
        
        scaler.step(optimizer)
        scaler.update()
        
        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
        num_batches += 1
        
        if batch_idx % config.print_freq == 0:
            acc = 100. * correct / total
            lr = optimizer.param_groups[0]['lr']
            avg_loss = total_loss / num_batches
            print(f'  Epoch {epoch:3d} [{batch_idx:4d}/{len(loader):4d}] | '
                  f'Loss: {avg_loss:.4f} | Acc: {acc:6.2f}% | LR: {lr:.7f}')
    
    scheduler.step()
    return total_loss / num_batches, 100. * correct / total


@torch.no_grad()
def validate(model, loader, criterion):
    """Validation"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    for inputs, targets in loader:
        inputs, targets = inputs.cuda(), targets.cuda()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        
        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
    
    return total_loss / len(loader), 100. * correct / total


def phase1_simclr_pretraining(backbone, config: SimCLRConfig):
    """Phase 1: SimCLR v2 self-supervised pre-training on 100k unlabeled images"""
    print("\n" + "="*100)
    print("PHASE 1: SimCLR v2 SELF-SUPERVISED PRE-TRAINING (BF16)".center(100))
    print("Training on 100,000 unlabeled STL-10 images".center(100))
    print("="*100)
    
    # Create SimCLR model
    simclr_model = HQAViTWithSimCLR(backbone, config).cuda()
    
    # Load unlabeled data
    print(f"\nüìÇ Loading STL-10 unlabeled data...")
    unlabeled_loader = get_stl10_unlabeled_loader(config)
    print(f"   Unlabeled: {len(unlabeled_loader.dataset):,} samples ({len(unlabeled_loader)} batches)")
    print(f"   Batch size: {config.simclr_batch_size}")
    print(f"   Two augmented views per image for contrastive learning")
    
    # Calculate effective learning rate with conservative scaling
    # For small batch size, use sqrt scaling instead of linear
    lr_scale = math.sqrt(config.simclr_batch_size / 256)
    effective_lr = config.simclr_base_lr * lr_scale
    
    # Setup optimizer with conservative learning rate
    optimizer = optim.AdamW(
        simclr_model.parameters(),
        lr=effective_lr,
        betas=(0.9, 0.999),
        weight_decay=config.weight_decay,
        eps=1e-8  # Increased eps for stability
    )
    
    # Cosine annealing with warmup
    main_scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=config.simclr_epochs - config.simclr_warmup_epochs,
        eta_min=1e-6
    )
    
    # Gradual warmup
    warmup_scheduler = optim.lr_scheduler.LambdaLR(
        optimizer,
        lr_lambda=lambda epoch: (epoch + 1) / config.simclr_warmup_epochs
    )
    
    scaler = GradScaler(enabled=config.use_amp)
    criterion = NTXentLoss(temperature=config.simclr_temperature)
    
    logger = TrainingLogger(config.log_dir, phase='simclr')
    
    print(f"\nüîß SimCLR Configuration (OPTIMIZED FOR STABILITY):")
    print(f"   Precision:        BF16 (bfloat16)")
    print(f"   Epochs:           {config.simclr_epochs} (warmup: {config.simclr_warmup_epochs})")
    print(f"   Batch size:       {config.simclr_batch_size}")
    print(f"   Base LR:          {config.simclr_base_lr}")
    print(f"   Scaled LR:        {effective_lr:.6f} (sqrt scaling)")
    print(f"   Temperature:      {config.simclr_temperature}")
    print(f"   Projection dim:   {config.projection_dim}")
    print(f"   Hidden dim:       {config.hidden_dim}")
    print(f"   Weight decay:     {config.weight_decay}")
    print(f"   Grad clip:        {config.max_grad_norm}")
    
    print(f"\n{'='*100}")
    print("SIMCLR TRAINING STARTED".center(100))
    print(f"{'='*100}\n")
    
    best_loss = float('inf')
    
    for epoch in range(1, config.simclr_epochs + 1):
        epoch_start = time.time()
        
        # Use appropriate scheduler
        if epoch <= config.simclr_warmup_epochs:
            current_scheduler = warmup_scheduler
        else:
            current_scheduler = main_scheduler
        
        try:
            train_loss = train_simclr_epoch(
                simclr_model, unlabeled_loader, optimizer, current_scheduler,
                scaler, criterion, config, epoch
            )
        except ValueError as e:
            print(f"\n‚ùå Training stopped: {e}")
            break
        
        epoch_time = time.time() - epoch_start
        current_lr = optimizer.param_groups[0]['lr']
        
        logger.log_epoch(epoch, {
            'train_loss': train_loss,
            'lr': current_lr,
            'epoch_time': epoch_time
        })
        
        print(f"\n{'='*100}")
        print(f"EPOCH {epoch}/{config.simclr_epochs} SUMMARY".center(100))
        print(f"{'='*100}")
        print(f"  Contrastive Loss: {train_loss:.4f}")
        print(f"  Learning Rate:    {current_lr:.6f}")
        print(f"  Epoch Time:       {epoch_time:.1f}s")
        
        if train_loss < best_loss:
            best_loss = train_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': simclr_model.backbone.state_dict(),
                'projection_head': simclr_model.projection_head.state_dict(),
                'loss': train_loss,
                'config': config,
            }, f"{config.checkpoint_dir}/simclr_best.pth")
            print(f"  üåü NEW BEST! Loss: {best_loss:.4f} (saved)")
        else:
            print(f"  üìä Best Loss: {best_loss:.4f}")
        
        print(f"{'='*100}\n")
        
        if epoch % 10 == 0:
            logger.plot_training_curves()
            logger.save_metrics()
    
    logger.save_metrics()
    logger.plot_training_curves()
    
    print(f"\n{'='*100}")
    print("SIMCLR PRE-TRAINING COMPLETE".center(100))
    print(f"{'='*100}")
    print(f"  Best Loss: {best_loss:.4f}")
    print(f"  Model saved to: {config.checkpoint_dir}/simclr_best.pth")
    print(f"{'='*100}\n")
    
    return simclr_model.backbone


def phase2_supervised_finetuning(backbone, config: SimCLRConfig, cifar100_acc: float):
    """Phase 2: Supervised fine-tuning on 5k labeled images"""
    print("\n" + "="*100)
    print("PHASE 2: SUPERVISED FINE-TUNING (BF16)".center(100))
    print("Training on 5,000 labeled STL-10 images".center(100))
    print("="*100)
    
    # Replace classification head for 10 classes
    backbone.head = nn.Linear(backbone.head.in_features, 10).cuda()
    
    # Load labeled data
    print(f"\nüìÇ Loading STL-10 labeled data...")
    train_loader, val_loader = get_stl10_supervised_loaders(config)
    print(f"   Train: {len(train_loader.dataset):,} samples ({len(train_loader)} batches)")
    print(f"   Val:   {len(val_loader.dataset):,} samples ({len(val_loader)} batches)")
    
    # Setup optimizer
    optimizer = optim.AdamW(
        backbone.parameters(),
        lr=config.finetune_base_lr,
        betas=(0.9, 0.999),
        weight_decay=config.weight_decay,
        eps=1e-8
    )
    
    # Cosine annealing
    main_scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=config.finetune_epochs - config.finetune_warmup_epochs,
        eta_min=1e-7
    )
    
    warmup_scheduler = optim.lr_scheduler.LambdaLR(
        optimizer,
        lr_lambda=lambda epoch: (epoch + 1) / config.finetune_warmup_epochs
    )
    
    scaler = GradScaler(enabled=config.use_amp)
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    
    logger = TrainingLogger(config.log_dir, phase='finetune')
    
    print(f"\nüîß Fine-tuning Configuration:")
    print(f"   Precision:        BF16 (bfloat16)")
    print(f"   Epochs:           {config.finetune_epochs} (warmup: {config.finetune_warmup_epochs})")
    print(f"   Batch size:       {config.finetune_batch_size}")
    print(f"   Base LR:          {config.finetune_base_lr:.6f}")
    print(f"   Weight decay:     {config.weight_decay}")
    print(f"   Label smoothing:  0.1")
    
    print(f"\n{'='*100}")
    print("SUPERVISED FINE-TUNING STARTED".center(100))
    print(f"{'='*100}\n")
    
    best_acc = 0.0
    best_epoch = 0
    
    for epoch in range(1, config.finetune_epochs + 1):
        epoch_start = time.time()
        
        current_scheduler = warmup_scheduler if epoch <= config.finetune_warmup_epochs else main_scheduler
        
        try:
            train_loss, train_acc = train_finetune_epoch(
                backbone, train_loader, optimizer, current_scheduler,
                scaler, criterion, config, epoch
            )
            
            val_loss, val_acc = validate(backbone, val_loader, criterion)
        except ValueError as e:
            print(f"\n‚ùå Training stopped: {e}")
            break
        
        epoch_time = time.time() - epoch_start
        current_lr = optimizer.param_groups[0]['lr']
        
        logger.log_epoch(epoch, {
            'train_loss': train_loss,
            'train_acc': train_acc,
            'val_loss': val_loss,
            'val_acc': val_acc,
            'lr': current_lr,
            'epoch_time': epoch_time
        })
        
        print(f"\n{'='*100}")
        print(f"EPOCH {epoch}/{config.finetune_epochs} SUMMARY".center(100))
        print(f"{'='*100}")
        print(f"  Train Loss: {train_loss:.4f}  |  Train Acc: {train_acc:6.2f}%")
        print(f"  Val Loss:   {val_loss:.4f}  |  Val Acc:   {val_acc:6.2f}%")
        print(f"  Learning Rate: {current_lr:.7f}")
        print(f"  Epoch Time: {epoch_time:.1f}s")
        
        if val_acc > best_acc:
            best_acc = val_acc
            best_epoch = epoch
            torch.save({
                'epoch': epoch,
                'model_state_dict': backbone.state_dict(),
                'val_acc': val_acc,
                'train_acc': train_acc,
                'cifar100_acc': cifar100_acc,
                'config': config,
            }, f"{config.checkpoint_dir}/best_finetuned.pth")
            print(f"  üåü NEW BEST! Val Acc: {best_acc:.2f}% (saved)")
        else:
            print(f"  üìä Best Val Acc: {best_acc:.2f}% (epoch {best_epoch})")
        
        print(f"{'='*100}\n")
        
        if epoch % 10 == 0:
            logger.plot_training_curves()
            logger.save_metrics()
    
    logger.save_metrics()
    logger.plot_training_curves()
    
    print(f"\n{'='*100}")
    print("SUPERVISED FINE-TUNING COMPLETE".center(100))
    print(f"{'='*100}")
    print(f"  Best Val Acc: {best_acc:.2f}% (epoch {best_epoch})")
    print(f"  Model saved to: {config.checkpoint_dir}/best_finetuned.pth")
    print(f"{'='*100}\n")
    
    return best_acc, best_epoch


def plot_final_comparison(config: SimCLRConfig, cifar100_acc: float, stl10_acc: float):
    """Plot final comparison across all stages"""
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # 1. Training pipeline visualization
    ax1 = axes[0]
    stages = ['CIFAR-100\nPretrained', 'SimCLR v2\n(100k unlabeled)', 'STL-10\nFine-tuned\n(5k labeled)']
    colors = ['gray', 'orange', 'green']
    
    y_pos = np.arange(len(stages))
    ax1.barh(y_pos, [1, 1, 1], color=colors, alpha=0.3, height=0.6)
    
    for i, (stage, color) in enumerate(zip(stages, colors)):
        ax1.text(0.5, i, stage, ha='center', va='center', 
                fontsize=12, fontweight='bold', color='black')
    
    ax1.set_xlim([0, 1])
    ax1.set_ylim([-0.5, 2.5])
    ax1.set_xticks([])
    ax1.set_yticks([])
    ax1.set_title('Semi-Supervised Learning Pipeline', fontsize=16, fontweight='bold')
    ax1.spines['top'].set_visible(False)
    ax1.spines['right'].set_visible(False)
    ax1.spines['bottom'].set_visible(False)
    ax1.spines['left'].set_visible(False)
    
    for i in range(len(stages) - 1):
        ax1.annotate('', xy=(0.5, i - 0.3), xytext=(0.5, i + 0.7),
                    arrowprops=dict(arrowstyle='->', lw=3, color='black'))
    
    # 2. Accuracy comparison
    ax2 = axes[1]
    methods = ['Supervised Only\n(CIFAR-100 init)', 'Semi-Supervised\n(SimCLR + 5k labels)']
    accuracies = [cifar100_acc, stl10_acc]
    colors_bar = ['steelblue', 'forestgreen']
    
    bars = ax2.bar(methods, accuracies, color=colors_bar, alpha=0.7, 
                   edgecolor='black', linewidth=2, width=0.5)
    
    for bar, acc in zip(bars, accuracies):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 1,
                f'{acc:.2f}%', ha='center', va='bottom', 
                fontsize=14, fontweight='bold')
    
    improvement = stl10_acc - cifar100_acc
    ax2.annotate(f'{improvement:+.2f}%\nimprovement', 
                xy=(1, stl10_acc), xytext=(0.5, cifar100_acc + improvement/2),
                ha='center', fontsize=12, color='darkgreen', fontweight='bold',
                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow', alpha=0.8))
    
    ax2.set_ylabel('Validation Accuracy (%)', fontsize=14, fontweight='bold')
    ax2.set_title('STL-10 Performance Comparison', fontsize=16, fontweight='bold')
    ax2.set_ylim([min(accuracies) - 5, max(accuracies) + 5])
    ax2.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plot_path = Path(config.log_dir) / 'final_comparison.png'
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"üìä Final comparison saved to: {plot_path}")


def main():
    print("\n" + "="*100)
    print("HQA-ViT ROBUSTNESS TEST: SEMI-SUPERVISED LEARNING ON STL-10".center(100))
    print("FIXED & OPTIMIZED - ALL BF16 OPERATIONS".center(100))
    print("="*100)
    
    config = SimCLRConfig()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print(f"\nüñ•Ô∏è  Device: {torch.cuda.get_device_name(0) if device.type == 'cuda' else 'CPU'}")
    print(f"    Precision: BF16 (bfloat16) for numerical stability")
    
    # Check BF16 support
    if torch.cuda.is_available() and not torch.cuda.is_bf16_supported():
        print("    ‚ö†Ô∏è  Warning: BF16 not supported on this GPU, falling back to FP32")
        config.use_amp = False
    
    # Windows-specific settings
    if os.name == 'nt':
        print("    ‚ö†Ô∏è  Windows detected - setting num_workers=0 and pin_memory=False")
        config.num_workers = 0
        config.pin_memory = False
    
    # Create directories
    Path(config.checkpoint_dir).mkdir(parents=True, exist_ok=True)
    Path(config.log_dir).mkdir(parents=True, exist_ok=True)
    
    # Load CIFAR-100 pretrained model
    print(f"\nüì¶ Loading CIFAR-100 pretrained model from: {config.pretrained_path}")
    
    if not os.path.exists(config.pretrained_path):
        raise FileNotFoundError(f"Pretrained model not found at {config.pretrained_path}")
    
    checkpoint = torch.load(config.pretrained_path, map_location='cpu')
    model_config = checkpoint.get('model_config', HQAViTConfig())
    
    # Create backbone
    backbone = HQAViT(model_config).to(device)
    backbone.load_state_dict(checkpoint['model_state_dict'])
    
    cifar100_acc = checkpoint.get('val_acc', 0.0)
    print(f"   ‚úÖ CIFAR-100 Validation Accuracy: {cifar100_acc:.2f}%")
    print(f"   üî¨ Testing robustness via transfer to STL-10")
    
    print(f"\nüìã Experiment Overview:")
    print(f"   1. Phase 1: SimCLR v2 self-supervised learning on 100k unlabeled STL-10 images")
    print(f"   2. Phase 2: Supervised fine-tuning on 5k labeled STL-10 images")
    print(f"   3. Goal: Test if CIFAR-100 features transfer well to larger, diverse images")
    
    print(f"\nüíæ Dataset Information:")
    print(f"   STL-10: 96x96 images (vs CIFAR 32x32)")
    print(f"   - 100,000 unlabeled images (for SimCLR)")
    print(f"   - 5,000 labeled training images")
    print(f"   - 8,000 labeled test images")
    print(f"   - 10 classes (airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck)")
    
    # Phase 1: SimCLR pre-training
    backbone_simclr = phase1_simclr_pretraining(backbone, config)
    
    # Phase 2: Supervised fine-tuning
    final_acc, best_epoch = phase2_supervised_finetuning(backbone_simclr, config, cifar100_acc)
    
    # Final comparison plot
    plot_final_comparison(config, cifar100_acc, final_acc)
    
    # Final summary
    print("\n" + "="*100)
    print("EXPERIMENT COMPLETE: ROBUSTNESS TEST RESULTS".center(100))
    print("="*100)
    print(f"\nüéØ Results Summary:")
    print(f"   CIFAR-100 Pretrained Accuracy:        {cifar100_acc:.2f}%")
    print(f"   STL-10 Final Accuracy:                {final_acc:.2f}%")
    print(f"   Transfer Performance:                 {final_acc - cifar100_acc:+.2f}%")
    print(f"   Best Epoch:                           {best_epoch}")
    
    print(f"\nüî¨ Robustness Analysis:")
    if final_acc > cifar100_acc:
        print(f"   ‚úÖ EXCELLENT: Model improved on STL-10 despite larger image size")
        print(f"   ‚úÖ Features learned on CIFAR-100 transfer well to different distribution")
        print(f"   ‚úÖ SimCLR v2 successfully leveraged unlabeled data")
    elif final_acc > cifar100_acc - 5:
        print(f"   ‚úî  GOOD: Model maintained performance on different dataset")
        print(f"   ‚úî  Features show reasonable robustness to domain shift")
    else:
        print(f"   ‚ö†Ô∏è  MODERATE: Some performance drop on larger images")
        print(f"   ‚ö†Ô∏è  May indicate overfitting to 32x32 resolution")
    
    print(f"\nüí° Key Insights:")
    print(f"   - SimCLR v2 utilized 100k unlabeled images (20x more than supervised)")
    print(f"   - Self-supervised learning helped adapt to STL-10's larger images")
    print(f"   - Only 5k labeled samples needed for final fine-tuning")
    print(f"   - Demonstrates strong transfer learning from CIFAR-100")
    print(f"   - BF16 precision ensured numerical stability throughout training")
    
    print(f"\nüìÅ Saved Files:")
    print(f"   SimCLR Model:        {config.checkpoint_dir}/simclr_best.pth")
    print(f"   Fine-tuned Model:    {config.checkpoint_dir}/best_finetuned.pth")
    print(f"   SimCLR Logs:         {config.log_dir}/simclr/")
    print(f"   Fine-tuning Logs:    {config.log_dir}/finetune/")
    print(f"   Final Comparison:    {config.log_dir}/final_comparison.png")
    
    print(f"\n{'='*100}\n")
    
    return final_acc


if __name__ == "__main__":
    # Set seeds for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True
        # Enable TF32 for faster computation on Ampere+ GPUs
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
    
    try:
        final_acc = main()
        print(f"‚úÖ Semi-supervised learning completed successfully!")
        print(f"   Final STL-10 accuracy: {final_acc:.2f}%")
        print(f"   All operations used BF16 for stability")
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Training interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Training failed with error: {e}")
        import traceback
        traceback.print_exc()